llama-cpp-python==0.2.90
pyyaml==6.0.1
watchdog==4.0.0

# For GPU support (CUDA), install with:
# CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
